```{r child = 'rmd_etc/rmd_options.Rmd'}
```

# 어떤 이야기를 가장 많이 했을까? - 단어 빈도 분석

문장은 단어의 연결로 이루어져 있습니다. 텍스트에 자주 사용된 단어를 파악하면 글쓴이가 어떤 내용을 강조했는지, 글의 핵심 주제와 글쓴이의 의도를 이해할 수 있습니다. 단어 빈도(Word Frequency)를 분석해 자주 사용된 단어를 알아내는 방법을 익혀보겠습니다.

> 설명 그림 삽입


## 기본적인 텍스트 전처리

문재인 대통령 출마 연설문을 이용해서 단어 빈도 분석을 하는 방법을 알아보자. 우선 연설문이 삽입 글자 담겨있는 `text_moon.txt`를 `readLines()`를 이용해 불러온 다음 분석에 적합하도록 전처리하겠습니다.


```{r, include=F}
raw_moon <- readLines("files/speech_moon.txt", encoding = "UTF-8")
head(raw_moon)
```

```{r, eval=F}
raw_moon <- readLines("speech_moon.txt", encoding = "UTF-8")
head(raw_moon)
```

> [참고] 실습 파일을 다운로드 하는 방법은 00쪽을 참고하세요(원문 출처: https://ko.wikisource.org/wiki/문재인_출마선언문).


### 불필요한 문자 제거하기

`raw_moon`의 출력 결과를 살펴보면 특수문자, 한자, 공백 등 분석의 대상이 되지 않는 문자들을 포함하고 있습니다. 분석에 앞서 이런 불필요한 문자들을 제거해야 합니다.
`stringr` 패키지는 텍스트 데이터를 처리하는데 유용한 함수들로 구성되어 있습니다. 텍스트 데이터를 다룰 때는 `stringr` 패키지의 함수들을 활용하게 되는데, 특히 전처리 작업에 자주  활용합니다.

가장 먼저 `stringr` 패키지의 `str_replace_all()`을 이용해 불필요한 여기도 수정 제거하는 방법을 알아보겠습니다. `str_replace_all()`은 특정한 규칙에 해당하는 문자를 다른 문자로 바꾸는 기능을 합니다. `string` 파라미터에는 처리할 텍스트 데이터를 입력하고, `pattern`에는 규칙을 입력합니다. `replacement`에는 이 규칙에 해당하는 문자를 무엇으로 바꿀지 입력합니다.

아래 코드에서 `pattern`에 입력된 `[^가-힣]`은 '한글이 아닌 모든 문자'를 동시에 수정 정규표현식입니다. `replacement`에는 공백을 입력했습니다. 따라서 이 함수는 한글 이외의 모든 문자를 공백으로 바꾸는 기능을 합니다.

```{r}
txt <- "치킨은!! 맛있다. xyz 정말 맛있다!@#"
txt

# install.packages("stringr")
library(stringr)
str_replace_all(string = txt, pattern = "[^가-힣]", replacement = " ")
```

이제 `str_replace_all()` 사용법을 익혔으니 `raw_moon`에 적용해 불필요한 문자를 제거하겠습니다.

```{r}
moon <- raw_moon %>%
  str_replace_all("[^가-힣]", " ")

head(moon)
```


---

> [알아두면 좋아요] 함수의 파라미터에는 순서가 정해져 있다. 파라미터 순서를 바꿔서 수정하면 어떻게 변할지 테스트해봅시다. 파라미터의 순서는 함수 설명 문서의 Arguments 항목을 보면 알 수 있습니다. `?str_replace_all` 처럼, 함수명 앞에 물음표를 넣어 Help 함수를 실행하면 R 스튜디오 우측 하단 Help 창에 설명 문서가 나타납니다. 

```{r, eval = F}
# 파라미터명 입력
str_replace_all(string = txt, pattern = "[^가-힣]", replacement = " ")

# 파라미터명 생략
str_replace_all(txt, "[^가-힣]", " ")
```

---

### 중복된 공백 제거하기

앞에서 출력한 결과를 보면 한글을 제외한 수정할 때마다 한 번씩 커밋. 문자를 공백으로 바꾸었기 때문에 여러 개의 공백이 중복되어 있습니다. `stringr` 패키지의 `str_squish()`를 이용하면 공백이 여러 번 반복 될 경우 하나만 남기고 제거할 수 있습니다. 아래 코드를 실행하면 공백이 몇 번 반복되든 한 개로 바뀌는 것을 확인할 수 있습니다.

```{r}
txt <- "치킨은   맛있다      정말 맛있다   "
txt
str_squish(txt)
```

이제 연설문에 `str_squish()`을 적용해 중복된 공백들을 한 개로 바꾸겠습니다.

```{r}
moon <- moon %>%
  str_squish()

head(moon)
```


### tibble 구조로 바꾸기

`moon`은 문자가 나열된 문자열 벡터(Character Vectors) 구조로 되어 있습니다. 문자열 벡터는 행에 들어있는 모든 내용을 출력하기 때문에, 긴 문자가 들어있으면 출력 결과를 알아보기 어렵습니다. 이럴 때, `dplyr` 패키지의 `as_tibble()`을 이용해 character를 tibble 구조로 변환하면 다루기 편리합니다.

```{r}
library(dplyr)
moon <- as_tibble(moon)
moon
```

tibble 구조로 변환한 `moon`을 출력하면 한 행에 한 문장 단락이 들어있고, 문장이 너무 길면 Console 창에 표현하기 적당할 정도로 일부만 출력한다는 것을 알 수 있습니다. `A tibble: 117 x 1`을 보면 이 데이터가 117개의 행과 1개의 열로 구성된다는 것을 알 수 있고, 그 아래를 보면 `moon`에 들어있는 변수명이 `value`이며, 문자 타입(chr)이라는 것을 알 수 있습니다.

> [참고] 지금은 문자가 없는 빈 행이 포함되어 있는데, 이는 원자료의 문장 사이에 줄 바뀜이 여러 번 사용되었기 때문입니다. 빈 행은 이후 단어를 추출하는 과정에서 제거됩니다.

`%>%`를 이용해 함수를 연결하면, 텍스트에서 한글만 남기고, 중복된 공백을 제거한 뒤, tibble로 변환하는 코드를 아래와 같이 간략하게 작성할 수 있습니다.

```{r}
moon <- raw_moon %>%
  str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
  str_squish() %>%                      # 중복된 공백 제거
  as_tibble()                           # tibble로 변환

moon
```


## 토큰화하기

#### 토큰화 {-}

텍스트는 단락, 문장, 단어, 형태소 등 다양한 단위로 나눌 수 있습니다. 이런 텍스트의 기본 단위를 토큰(token)이라고 합니다. 기본적인 전처리 작업이 끝나면 텍스트를 분석 목적에 따라 토큰으로 나누는 작업을 하게 되는데, 이를 토큰화(tokenizing)라고 합니다.

`tidytext` 패키지의 `unnest_tokens()`을 이용하면 텍스트를 토큰화할 수 있습니다. `input`에 분석 대상이 되는 변수명을 입력하고, `output`에는 함수의 결과로 만들 변수명을 입력합니다. `token`에는 텍스트를 무엇을 기준으로 나눌지, 기준 값을 입력하면 됩니다. 문장 기준으로 나누려면 `"sentences"`, 띄어쓰기 기준으로 나누려면 "words", 글자 기준으로 나누려면 `"characters"`를 입력하면 됩니다. 아래 코드의 출력 결과를 보면, 텍스트가 토큰으로 나뉘어 각 행으로 구성된 것을 확인할 수 있습니다. 

> [참고] `unnest_tokens()`에는 tibble이나 데이터 프레임 구조의 변수를 입력해야 하니 주의하세요.

```{r}
text <- tibble(value = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.")
text

# install.packages("tidytext")
library(tidytext)

# 문장 기준 토큰화
text %>% 
  unnest_tokens(input = value,         # 분석 대상 
                output = word,         # 출력 변수명 
                token = "sentences")   # 문장 기준

# 띄어쓰기 기준 토큰화
text %>% 
  unnest_tokens(input = value,    
                output = word,    
                token = "words")       # 띄어쓰기 기준

# 문자 기준 토큰화
text %>% 
  unnest_tokens(input = value,     
                output = word,     
                token = "characters")  # 문자 기준
```

`unnest_tokens()` 사용법을 익혔으니 이제 연설문에 적용해보겠습니다. 어떤 단어가 많이 사용 되었는지 알아보는 것이 목적이니, 연설문을 띄어쓰기 기준으로 토큰화하겠습니다.
```{r}
word_space <- moon %>% 
  unnest_tokens(
    input = value,
    output = word,
    token = "words")

word_space
```


## 단어 빈도 분석하기

### 단어 빈도 구하기

연설문을 단어 기준으로 나누었으니, 이제 빈도를 구해 어떤 단어가 많이 사용되었는지 알아보겠습니다. `dplyr` 패키지의 `count()`를 이용하면 각 단어의 빈도를 알 수 있습니다. `count()`에 `sort = T`를 추가하면 단어가 많이 사용된 순으로 정렬됩니다.

```{r}
word_space <- word_space %>%
  count(word, sort = T) 

word_space
```

출력 결과를 보면 연설문에 어떤 단어가 얼마나 많이 사용되었는지 알 수 있습니다. `A tibble: 1,440 x 2`를 보면 총 1,440개의 단어로 구성되어 있다는 것을 알 수 있습니다.

> [참고] 대부분의 단어가 '합니다', '있습니다'처럼 서술어로 구성되어 있는데, 이는 텍스트를 토큰화할 때 띄어쓰기를 기준으로 삼았기 때문입니다. 텍스트의 의미를 파악하려면 
텍스트의 의미를 결정하는 단위로 토큰화해야 합니다. 이 방법에 대해서는 뒤에서 자세히 다루겠습니다.


### 한 글자로 된 단어 제거하기

`word_space` 출력 결과를 보면 "수", "등" 처럼 한 글자로 된 단어가 여럿 포함되어 있습니다. 한 글자로 된 단어는 문장에서 어떻게 사용됐는지 전후 맥락을 알지 못하면 의미를 해석할 수 없기 때문에 분석에서 제외하는 게 좋습니다.
`stringr` 패키지의 `str_count()`는 글자 수를 구해주는데, 이 함수를 `filter()`에 적용하면 두 글자 이상으로 된 단어만 남길 수 있습니다. 아래 코드를 실행하면 한 글자로 된 단어가 제외되어 1,384개 단어가 남은 것을 확인할 수 있습니다.

```{r}
str_count("배")
str_count("사과")

# 두 글자 이상만 남기기
word_space <- word_space %>% 
  filter(str_count(word) > 1)
  
word_space
```

`%>%`를 이용해 함수를 연결하면 단어의 빈도를 구해 내림차순 정렬한 뒤 두 글자 이상만 남기는 코드를 아래와 같이 간략하게 작성할 수 있습니다.
```{r, eval = F}
word_space <- word_space %>%
  count(word, sort = T) %>% 
  filter(str_count(word) > 1)
```


### 자주 사용된 단어 추출하기

연설문에서 어떤 단어가 가장 많이 사용되었는지 알아보기 위해, 1,384개 단어로 구성된 `word_space`에서 빈도가 높은 상위 20개를 추출하겠습니다. `word_space`는 빈도가 높은 순으로 정렬되어 있기 때문에 `head()`를 이용하면 됩니다.

```{r}
top20 <- word_space %>% 
  head(20)

top20
```


### 막대 그래프 만들기

어떤 단어가 얼마나 많이 사용되었는지 쉽게 알아볼 수 있도록 `ggplot2` 패키지를 이용해 막대 그래프를 만들겠습니다. 단어별 빈도로 집계된 요약표 형태의 데이터를 활용하기 때문에 `geom_col()`을 이용하면 간단히 막대 그래프를 만들 수 있습니다.

```{r}
# install.packages("ggplot2")
library(ggplot2)

ggplot(data = top20,
       aes(x = reorder(word, n),           # 빈도순 정렬
           y = n)) +
  geom_col() +
  coord_flip() +                           # 회전
  geom_text(aes(label = n), hjust = -0.3)  # 빈도 표시
```


### 워드 클라우드 만들기

워드 클라우드(Word cloud)는 단어의 빈도를 구름 모양으로 표현한 그래프입니다. 워드 클라우드를 만들면 단어의 빈도에 따라 글자의 크기와 색깔이 다르게 표현되기 때문에 어떤 단어가 얼마나 많이 사용됐는지 한눈에 파악할 수 있습니다. `wordcloud` 패키지를 이용해 `word_space`를 워드 클라우드로 시각화해보겠습니다.

<!-- 워드클라우드 패키지 수정 예정 -->

```{r, cache=T}
# install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
pal <- brewer.pal(9,"Blues")[5:9]   #  색상 목록 생성

set.seed(1234)                      #  난수 고정
wordcloud(words = word_space$word,  #  단어
          freq = word_space$n,      #  빈도
          min.freq = 3,             #  최소 단어 빈도
          max.words = 200,          #  표현 단어 수
          random.order = F,         #  고빈도 단어 중앙 배치
          rot.per = .1,             #  회전 단어 비율
          scale = c(4, 0.5),        #  단어 크기 범위
          colors = pal)             #  색상 목록
```

<!-- ![](../img/image_wordcloud_noun.png) -->

> [참고] 워드클라우드를 만들 때는 상위 20개 단어를 추출한 `top20`가 아니라 모든 단어 빈도를 나타낸 `word_space` 활용해야하니 주의하세요

---
> [알아두면 좋아요] 워드 클라우드는 좋은 그래프인가?

> 워드 클라우드는 디자인이 아름답기 때문에 텍스트 데이터를 시각화하는데 많이 사용됩니다. 하지만 단어의 빈도를 크기와 색깔로 표현하기 때문에 각 단어가 정확히 몇 번 사용되었는지 알 수 없다는 문제가 있습니다. 또한 단어들이 어지럽게 배치되어 있기 때문에 어떤 단어가 다른 단어보다 얼마나 많이 사용되었는지 비교하기 어렵습니다. 텍스트를 아름답게 표현하는 심미적인 목적이라면 사용해도 좋겠지만, 정확하게 분석하려는 목적이라면 막대 그래프를 이용하길 권합니다.
---

